# https://docs.wandb.ai/guides/sweeps/sweep-config-keys/#distribution-options-for-random-and-bayesian-search
program: interactive_navigation/scripts/rsl_rl/train.py
# program: scripts/rsl_rl/train.py
description: "Sweep over hyperparameters for metra ppo ant"
method: bayes  # or 'grid', 'random'
metric:
  name: Metra/reward_mean  # wandb metric to optimize
  goal: maximize  # 'minimize' or 'maximize' if optimizing a loss
parameters:
# env
  env.scene.num_envs:
    values: [256, 512, 1024, 2048, 4096]
  agent.num_transitions_per_episode:
    min: 4096 # 4096 * 1
    max: 409600 # 4096 * 100
    distribution: int_uniform
  agent.empirical_normalization:
    values: [True]
  agent.seed:
    min: 0
    max: 1000
# metra
  agent.metra.batch_size:
    values: [128, 256, 512, 1024]
  agent.metra.replay_buffer_size_total:
    min: 100000 #100k
    max: 2500000 #2.5M
    distribution: int_uniform
  agent.metra.num_sgd_steps_metra:
    min: 10
    max: 250
    distribution: int_uniform
  agent.metra.num_metra_learning_epochs:
    min: 1
    max: 20
    distribution: int_uniform
  agent.metra.replay_buffer_num_envs:
    values: [8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]
  # agent.metra.non_metra_reward_scale:
  #   min: 1.0
  #   max: 3.0
  #   distribution: uniform
  agent.metra.skill_dim:
    values: [2, 4]
# algorithm
  agent.algorithm.gamma:
    min: 0.95
    max: 0.995
    distribution: log_uniform_values
  agent.algorithm.entropy_coef:
    min: 0.0025
    max: 0.01
    distribution: log_uniform_values
  agent.algorithm.learning_rate:
    min: 5e-5
    max: 25e-4
    distribution: log_uniform_values
  agent.algorithm.clip_param:
    min: 0.1
    max: 0.3
    distribution: uniform

    # TODO gamma and other ppo params


command:
  - /isaac-sim/python.sh
  - ${program}
  - --task=Isaac-METRA-ANT-v0
  - --logger=wandb
  - --headless
  - --video
  - --video_length=100
  - --video_interval=50000
  - ${args_no_hyphens}
